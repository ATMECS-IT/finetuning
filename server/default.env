HUGGINGFACE_TOKEN=huggingface_access_token
PROJECT_NAME=default_project
MODEL_NAME=gpt2
TOKENIZER=gpt2
CACHE_DIR=./models
USE_AUTH_TOKEN=False
PAD_TOKEN=<eos>
MAX_LENGTH=512

TRAIN_DATA_PATH=./data/train1.jsonl
DATASET_TYPE=mcq_jsonl
DATA_FORMAT=jsonl

# Optional: Separate validation and test files
VAL_DATA_PATH=
TEST_DATA_PATH=

# If separate files not provided, split ratios from train file
TRAIN_RATIO=0.8
VAL_RATIO=0.1
TEST_RATIO=0.1
SPLIT_SEED=42

# Fine tuning technique
TECHNIQUE=lora

# LoRA Hyperparameters
LORA_R=8
LORA_ALPHA=16
LORA_DROPOUT=0.1
LORA_TARGET_MODULES=auto
LORA_BIAS=none

# Quantization Settings (for QLoRA)
LOAD_IN_4BIT=False
BNB_4BIT_COMPUTE_DTYPE=float16
BNB_4BIT_USE_DOUBLE_QUANT=False
BNB_4BIT_QUANT_TYPE=nf4

EPOCHS=3
BATCH_SIZE=4
LEARNING_RATE=3e-4
GRAD_ACC_STEPS=4
DEVICE=auto
SAVE_STRATEGY=epoch
EVAL_STEPS=100
PACKING=False

RESUME_CHECKPOINT=

# Metrics to track (comma-separated)
TRACK_METRICS=accuracy,f1,precision,recall,rouge,bleu
# Add rouge,bleu only if doing text generation
LOG_INTERVAL=10
METRICS_PATH=./results/metrics.json

PROMPTS_FILE=./prompts.txt
COMPARISON_MAX_TOKENS=150
COMPARISON_TEMPERATURE=0.7
COMPARISON_TOP_P=0.9
COMPARISON_DO_SAMPLE=True

OUTPUT_DIR=./results/output
REPORT_PATH=./results/report.json
