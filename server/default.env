
HUGGINGFACE_TOKEN=huggingface_access_token
PROJECT_NAME=default_project
MODEL_NAME=gpt2
TOKENIZER=gpt2
CACHE_DIR=./models
USE_AUTH_TOKEN=False
PAD_TOKEN=<eos>
MAX_LENGTH=512

TRAIN_DATA_PATH=./data/train.jsonl
DATASET_TYPE=instruction_jsonl
DATA_FORMAT=jsonl

# Fine tuning technique (sft, peft, lora, qlora, etc.)
TECHNIQUE=lora

LORA_R=8
LORA_ALPHA=16
LORA_DROPOUT=0.1
LORA_TARGET_MODULES=auto
LORA_BIAS=none

# LoRA Hyperparameters
LORA_R=8
LORA_ALPHA=16
LORA_DROPOUT=0.1
LORA_TARGET_MODULES=auto
LORA_BIAS=none

# Quantization Settings (for QLoRA)
LOAD_IN_4BIT=False
BNB_4BIT_COMPUTE_DTYPE=float16
BNB_4BIT_USE_DOUBLE_QUANT=False
BNB_4BIT_QUANT_TYPE=nf4

EPOCHS=3
BATCH_SIZE=4
LEARNING_RATE=3e-4
GRAD_ACC_STEPS=4
DEVICE=auto
SAVE_STRATEGY=epoch

RESUME_CHECKPOINT=

METRICS=loss,accuracy
LOG_INTERVAL=10
METRICS_PATH=./results/metrics.json

OUTPUT_DIR=./results/output
REPORT_PATH=./results/report.json
